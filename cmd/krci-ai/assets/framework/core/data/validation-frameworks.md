# Business Framework Validation Library

Purpose: Provide methodologically grounded validation frameworks for project brief enhancement. Each framework uses established business methodologies to validate specific aspects of project briefs with evidence-based approaches.

## Problem Validation Frameworks

<problem_validation>

### Lean Startup Problem-Solution Fit Assessment

Methodology: Eric Ries Lean Startup approach for validating problem-solution alignment.

Application: Use for Problem Statement and Opportunity sections validation.

Validation Process:

1. Problem Hypothesis Validation
   - Define problem hypothesis with specific user segment
   - Identify measurable problem indicators (support tickets, workarounds, time spent)
   - Validate problem urgency using customer interview evidence
   - Quantify problem frequency and impact

2. Solution Hypothesis Validation
   - Define minimum viable solution approach
   - Test solution desirability with target users
   - Validate solution feasibility with technical constraints
   - Assess solution viability with business model fit

3. Evidence Requirements
   - Customer interview data (minimum 10 target users)
   - Quantified problem metrics (costs, time, frequency)
   - Competitive analysis of existing solutions
   - Technical feasibility assessment

Validation Questions:

- Is this problem urgent enough for users to seek solutions?
- Do users currently pay (time/money) for workarounds?
- Does our solution approach address the root cause?
- Is the solution technically and commercially viable?
</problem_validation>

### Six Sigma Root Cause Analysis (5 Whys)

**Methodology**: Six Sigma DMAIC approach using 5 Whys technique for root cause identification.

**Application**: Use for Problem Statement depth validation and constraint analysis.

**Validation Process**:

1. **Problem Statement Definition**
   - State observable problem with quantified impact
   - Ask "Why does this problem occur?" - record first cause
   - Ask "Why does [first cause] occur?" - record second cause
   - Continue for minimum 5 levels of "why" questions
   - Validate each cause level with supporting evidence

2. **Evidence Requirements**
   - Data supporting each "why" level
   - Process documentation showing cause relationships
   - Historical data demonstrating problem persistence
   - Stakeholder confirmation of root causes

**Validation Questions**:

- Is each "why" level supported by evidence?
- Do the root causes align with proposed solution approach?
- Are root causes within our control to address?
- Have similar root causes been successfully addressed elsewhere?

### Impact-Frequency Problem Assessment

**Methodology**: Risk management matrix approach for problem prioritization validation.

**Application**: Use for Problem Statement prioritization and resource allocation validation.

**Validation Process**:

1. **Impact Assessment** (Scale 1-5)
   - User productivity impact
   - Business revenue impact
   - Operational cost impact
   - Strategic goal alignment impact

2. **Frequency Assessment** (Scale 1-5)
   - How often problem occurs
   - Number of users affected
   - Trend analysis (increasing/stable/decreasing)
   - Seasonal or cyclical patterns

3. **Priority Matrix Calculation**
   - Priority Score = Impact Ã— Frequency
   - High Priority: 16-25 (urgent action required)
   - Medium Priority: 6-15 (planned action needed)
   - Low Priority: 1-5 (monitor and defer)

**Evidence Requirements**:

- Quantified impact metrics with historical data
- User activity data showing frequency patterns
- Trend analysis over minimum 6-month period
- Comparative analysis with other organizational priorities

## User Validation Frameworks

<user_validation>

### Jobs-to-be-Done Validation (Clayton Christensen)

**Methodology**: Clayton Christensen's Jobs-to-be-Done framework for user need validation.

**Application**: Use for Target Users section validation and use case prioritization.

**Validation Process**:

1. **Job Statement Formulation**
   - When I [situation], I want to [motivation], so I can [expected outcome]
   - Validate job statement with target user interviews
   - Identify functional, emotional, and social job dimensions
   - Map job executor demographics and context

2. **Job Importance Assessment**
   - Rate job importance (1-10) from user perspective
   - Validate importance with willingness to pay data
   - Assess job frequency and urgency
   - Identify job outcome priorities

3. **Current Satisfaction Assessment**
   - Rate current solution satisfaction (1-10)
   - Identify specific satisfaction gaps
   - Map current solution journey and pain points
   - Validate dissatisfaction with switching behavior

4. **Opportunity Score Calculation**
   - Opportunity = Importance + max(Importance - Satisfaction, 0)
   - High Opportunity: 15+ (strong market opportunity)
   - Medium Opportunity: 12-15 (moderate market opportunity)
   - Low Opportunity: <12 (limited market opportunity)

**Evidence Requirements**:

- User interview data with job statement validation
- Quantified importance and satisfaction ratings
- Current solution usage and switching behavior data
- Demographic and contextual data for job executors

**Validation Questions**:

- Is the job statement validated by target users?
- Do users actively seek better solutions for this job?
- Is there sufficient dissatisfaction with current solutions?
- Does our solution approach align with job outcome priorities?

### Design Thinking User Journey Validation

**Methodology**: Design Thinking approach for user experience validation and empathy mapping.

**Application**: Use for Target Users section validation and user experience requirements.

**Validation Process**:

1. **Journey Stage Mapping**
   - Map complete user journey from awareness to outcome
   - Identify all touchpoints and interactions
   - Document user actions, thoughts, and emotions at each stage
   - Validate journey accuracy with user observation/interviews

2. **Pain Point Identification**
   - Identify specific pain points at each journey stage
   - Quantify pain point impact (time, effort, frustration)
   - Validate pain points with user evidence
   - Prioritize pain points by severity and frequency

3. **Opportunity Identification**
   - Identify improvement opportunities at each stage
   - Assess opportunity impact on user satisfaction
   - Validate opportunity desirability with users
   - Map opportunities to business value potential

4. **Empathy Map Creation**
   - What users Say (quotes and defining words)
   - What users Think (thoughts and beliefs)
   - What users Do (actions and behaviors)
   - What users Feel (emotions and feelings)

**Evidence Requirements**:

- User observation data or detailed user interviews
- Journey stage documentation with user validation
- Quantified pain point impact measurements
- Empathy map elements validated with user research

**Validation Questions**:

- Is the user journey accurately mapped and validated?
- Are pain points quantified and evidenced?
- Do improvement opportunities align with user priorities?
- Does the empathy map reflect authentic user perspectives?

### Persona Demographic Validation

**Methodology**: Data-driven persona validation using demographic and behavioral evidence.

**Application**: Use for Target Users section accuracy and market sizing validation.

**Validation Process**:

1. **Demographic Profile Validation**
   - Age, gender, education, income, role, industry
   - Geographic distribution and cultural factors
   - Technology adoption and digital behavior patterns
   - Validate demographics with market research data

2. **Behavioral Pattern Validation**
   - Usage frequency and session duration patterns
   - Device and platform preferences
   - Feature adoption and engagement metrics
   - Validate patterns with user analytics data

3. **Needs and Goals Validation**
   - Primary goals and success criteria
   - Secondary goals and nice-to-have outcomes
   - Barriers and constraints to goal achievement
   - Validate needs with user interview data

4. **Market Size Validation**
   - Total addressable market (TAM) calculation
   - Serviceable addressable market (SAM) estimation
   - Serviceable obtainable market (SOM) projection
   - Validate sizing with industry reports and data

**Evidence Requirements**:

- Market research data supporting demographic profiles
- User analytics data validating behavioral patterns
- User interview data supporting needs and goals
- Industry data supporting market size calculations

**Validation Questions**:

- Are demographic profiles supported by market data?
- Do behavioral patterns align with analytics evidence?
- Are user needs validated through direct user research?
- Is market sizing based on credible industry data?

</user_validation>

## Success Metrics Validation Frameworks

<metrics_validation>

### SMART Criteria Assessment

**Methodology**: SMART criteria framework for success metrics validation and quality assurance.

**Application**: Use for Success Metrics section validation and goal setting quality control.

**Validation Process**:

1. **Specific Validation**
   - Does the metric clearly define what will be measured?
   - Is the metric scope bounded and unambiguous?
   - Can stakeholders understand the metric without interpretation?
   - Is the metric aligned with project objectives?

2. **Measurable Validation**
   - Can the metric be quantified with specific numbers?
   - Are measurement tools and data sources identified?
   - Is the metric trackable throughout project lifecycle?
   - Are measurement intervals and reporting frequency defined?

3. **Achievable Validation**
   - Is the target realistic given available resources?
   - Are there precedents for similar metric achievements?
   - Have constraints and limitations been considered?
   - Is the metric within team/organization control or influence?

4. **Relevant Validation**
   - Does the metric align with business objectives?
   - Will achieving this metric create meaningful business value?
   - Is the metric important to key stakeholders?
   - Does the metric contribute to strategic goals?

5. **Time-bound Validation**
   - Is there a clear deadline or timeframe?
   - Are there intermediate milestones and checkpoints?
   - Is the timeline realistic for metric achievement?
   - Are dependencies and risks to timeline identified?

**Evidence Requirements**:

- Historical performance data for achievability assessment
- Measurement tool and data source documentation
- Stakeholder confirmation of relevance and importance
- Timeline feasibility analysis with dependency mapping

**Validation Questions**:

- Can this metric be measured accurately and consistently?
- Is the target achievable within specified constraints?
- Will achieving this metric create meaningful value?
- Is the timeline realistic given project dependencies?

### OKR Alignment Validation

**Methodology**: Objectives and Key Results framework for strategic alignment validation.

**Application**: Use for Success Metrics section strategic alignment and goal hierarchy validation.

**Validation Process**:

1. **Objective Alignment Validation**
   - Does project objective align with organizational OKRs?
   - Is the objective inspirational and directionally correct?
   - Does the objective contribute to higher-level strategic goals?
   - Is the objective clear enough to guide decision-making?

2. **Key Result Quality Validation**
   - Are key results measurable and outcome-focused?
   - Do key results collectively indicate objective achievement?
   - Are key results ambitious yet achievable (60-70% confidence)?
   - Do key results avoid activity-based metrics?

3. **OKR Hierarchy Validation**
   - How does project OKR cascade from organizational OKRs?
   - Are there conflicts with other team/department OKRs?
   - Is the OKR appropriately scoped for project level?
   - Does the OKR enable downstream team OKRs?

4. **Success Probability Assessment**
   - What's the confidence level for achieving each key result?
   - What are the key risks to OKR achievement?
   - What resources and dependencies are required?
   - How will progress be measured and reported?

**Evidence Requirements**:

- Organizational OKR documentation and alignment mapping
- Historical performance data for confidence assessment
- Resource availability and dependency analysis
- Risk assessment with mitigation strategies

**Validation Questions**:

- Do project OKRs clearly cascade from organizational strategy?
- Are key results appropriately ambitious and measurable?
- Is there organizational alignment and resource commitment?
- How will OKR progress be tracked and communicated?

### Leading/Lagging Indicator Analysis

**Methodology**: Performance measurement theory for balanced metric portfolio validation.

**Application**: Use for Success Metrics section balance and predictive capability validation.

**Validation Process**:

1. **Leading Indicator Identification**
   - Identify metrics that predict future outcomes
   - Validate leading indicators with historical correlation data
   - Assess leading indicator controllability and actionability
   - Define leading indicator measurement frequency

2. **Lagging Indicator Validation**
   - Identify outcome metrics that measure final results
   - Validate lagging indicators as true measures of success
   - Assess lagging indicator accuracy and reliability
   - Define lagging indicator measurement and reporting process

3. **Balance Assessment**
   - Is there appropriate balance between leading and lagging indicators?
   - Do leading indicators provide early warning capability?
   - Do lagging indicators measure ultimate project success?
   - Are there gaps in the measurement framework?

4. **Predictive Relationship Validation**
   - What's the correlation between leading and lagging indicators?
   - What's the time lag between leading indicator changes and outcome impact?
   - Are there external factors that affect the relationship?
   - How reliable is the predictive relationship?

**Evidence Requirements**:

- Historical data showing leading/lagging indicator relationships
- Correlation analysis and statistical validation
- External factor analysis and impact assessment
- Measurement system reliability and accuracy validation

**Validation Questions**:

- Do leading indicators reliably predict lagging outcomes?
- Is there balanced coverage of controllable and outcome metrics?
- Can the team act on leading indicator signals effectively?
- Are lagging indicators true measures of project success?

</metrics_validation>

## Business Value Validation Frameworks

<business_validation>

### Business Model Canvas Validation

**Methodology**: Alexander Osterwalder's Business Model Canvas for business value validation.

**Application**: Use for Opportunity section business model validation and value creation assessment.

**Validation Process**:

1. **Value Proposition Validation**
   - What specific value does the project create for customers?
   - How does the value proposition address customer jobs, pains, and gains?
   - Is the value proposition differentiated from alternatives?
   - Can the value proposition be delivered sustainably?

2. **Customer Segment Validation**
   - Are customer segments clearly defined and reachable?
   - Is there evidence of customer demand for the value proposition?
   - Are customer segments large enough to justify investment?
   - Do customer segments align with organizational capabilities?

3. **Revenue Model Validation**
   - How will the project generate revenue or cost savings?
   - Are revenue assumptions validated with customer evidence?
   - Is the revenue model sustainable and scalable?
   - What's the timeline to revenue realization?

4. **Cost Structure Analysis**
   - What are the key costs associated with value delivery?
   - Are cost estimates based on reliable data and assumptions?
   - How do costs scale with customer growth?
   - What's the path to positive unit economics?

**Evidence Requirements**:

- Customer interview data validating value proposition
- Market research supporting customer segment assumptions
- Financial model with validated assumptions
- Cost analysis with scaling scenarios

**Validation Questions**:

- Is there validated customer demand for the value proposition?
- Are revenue assumptions based on customer evidence?
- Is the business model economically viable and scalable?
- What are the key risks to business model success?

### Value Proposition Canvas Fit Assessment

**Methodology**: Alexander Osterwalder's Value Proposition Canvas for customer-solution fit validation.

**Application**: Use for Opportunity section customer-solution alignment and value creation validation.

**Validation Process**:

1. **Customer Profile Validation**
   - **Customer Jobs**: Functional, emotional, social jobs customers are trying to accomplish
   - **Pains**: Bad outcomes, obstacles, risks customers experience or fear
   - **Gains**: Benefits customers want, expect, desire, or would be surprised by

2. **Value Map Validation**
   - **Products & Services**: List of products and services the value proposition builds on
   - **Pain Relievers**: How products/services alleviate specific customer pains
   - **Gain Creators**: How products/services create customer gains

3. **Fit Assessment**
   - **Problem-Solution Fit**: Do pain relievers address important customer pains?
   - **Product-Market Fit**: Do gain creators deliver outcomes customers want?
   - **Value Proposition Fit**: Is there strong alignment between customer needs and solution benefits?

4. **Evidence Collection**
   - Customer interview validation of jobs, pains, and gains
   - Solution testing with target customers
   - Customer willingness to pay evidence
   - Customer switching behavior validation

**Evidence Requirements**:

- Customer research validating jobs, pains, and gains
- Solution prototype testing with customer feedback
- Customer willingness to pay and value perception data
- Competitive analysis of alternative solutions

**Validation Questions**:

- Are customer jobs, pains, and gains validated with evidence?
- Do solution features directly address important customer needs?
- Is there strong product-market fit based on customer evidence?
- What's the strength of the value proposition compared to alternatives?

### ROI Calculation Validation

**Methodology**: Financial analysis framework for return on investment validation and business case assessment.

**Application**: Use for Opportunity section financial justification and business case validation.

**Validation Process**:

1. **Investment Cost Analysis**
   - Development costs (personnel, tools, infrastructure)
   - Implementation costs (deployment, training, change management)
   - Operational costs (maintenance, support, ongoing resources)
   - Opportunity costs (alternative investment options)

2. **Benefit Quantification**
   - Revenue generation (new sales, upselling, market expansion)
   - Cost savings (efficiency, automation, waste reduction)
   - Risk mitigation (avoided costs, compliance, security)
   - Strategic benefits (market position, capability building)

3. **ROI Calculation Methods**
   - **Simple ROI**: (Benefits - Investment) / Investment Ã— 100
   - **Net Present Value (NPV)**: Present value of future cash flows minus initial investment
   - **Internal Rate of Return (IRR)**: Discount rate that makes NPV equal to zero
   - **Payback Period**: Time required to recover initial investment

4. **Sensitivity Analysis**
   - Best case, worst case, most likely scenario analysis
   - Key assumption impact on ROI calculations
   - Break-even analysis and threshold identification
   - Risk factor quantification and mitigation costs

**Evidence Requirements**:

- Detailed cost breakdown with vendor quotes and resource estimates
- Benefit quantification with historical data and industry benchmarks
- Financial projections with assumptions documentation
- Sensitivity analysis with scenario modeling

**Validation Questions**:

- Are cost estimates comprehensive and based on reliable data?
- Are benefit projections conservative and evidenced?
- Does the ROI meet organizational investment criteria?
- What are the key risks to ROI achievement and how can they be mitigated?

</business_validation>

## Framework Selection Guide

<selection_guide>

### Problem Statement Validation

- **Primary**: Lean Startup Problem-Solution Fit Assessment
- **Secondary**: Six Sigma Root Cause Analysis (5 Whys)
- **Situational**: Impact-Frequency Assessment (for prioritization)

### Target Users Validation

- **Primary**: Jobs-to-be-Done Validation
- **Secondary**: Design Thinking User Journey Validation
- **Situational**: Persona Demographic Validation (for market sizing)

### Success Metrics Validation

- **Primary**: SMART Criteria Assessment
- **Secondary**: OKR Alignment Validation
- **Situational**: Leading/Lagging Indicator Analysis (for balanced scorecards)

### Business Value Validation

- **Primary**: Value Proposition Canvas Fit Assessment
- **Secondary**: ROI Calculation Validation
- **Situational**: Business Model Canvas Validation (for new business models)

</selection_guide>

## Validation Confidence Levels

<confidence_levels>

### High Confidence (80-100%)

- Multiple validation frameworks applied
- Quantified evidence from primary sources
- Historical data supporting assumptions
- Expert validation and peer review

### Medium Confidence (60-79%)

- Primary validation framework applied
- Mix of quantified and qualitative evidence
- Industry benchmark data available
- Stakeholder validation completed

### Low Confidence (40-59%)

- Basic validation framework applied
- Limited quantified evidence
- Assumptions based on secondary research
- Expert opinion without validation

### Very Low Confidence (<40%)

- No formal validation applied
- Assumptions without supporting evidence
- No stakeholder validation
- High uncertainty in key assumptions

</confidence_levels>

## Usage Guidelines

<usage_guidelines>

### When to Apply Validation Frameworks

1. **Always Apply**: During initial project brief creation
2. **Apply Selectively**: When assumptions are challenged or evidence is weak
3. **Reapply**: When project scope, market, or constraints change significantly
4. **Skip**: When assumptions are well-validated with strong evidence

### Evidence Standards

- **Primary Evidence**: Customer interviews, user research, first-party data
- **Secondary Evidence**: Industry reports, competitive analysis, expert opinions
- **Quantified Evidence**: Numerical data, measurements, calculations
- **Qualitative Evidence**: User feedback, expert insights, observations

### Validation Documentation

- Document validation method used and rationale for selection
- Record evidence sources and confidence levels
- Track assumptions and validation status
- Update validation as new evidence becomes available
</usage_guidelines>
