# KubeRocketAI Framework Bundle

**Generated:** 2025-08-14 23:28:31 EEST
**Purpose:** Complete framework bundle for web chat tools (ChatGPT, Claude Web, Gemini Pro)

## Usage Instructions

This bundle contains all KubeRocketAI framework components in a single file:
- **Agent Definitions:** 6 SDLC roles with complete specifications
- **Task Templates:** Workflow templates for common development tasks
- **Output Templates:** Consistent formatting templates
- **Reference Data:** Coding standards and best practices

### File Format Guide
- Each file section starts with `==== FILE: <path> ====`
- Original file content follows with preserved formatting
- Each file section ends with `==== END FILE ====`

### For LLM Understanding
When working with this bundle:
1. Each agent represents a specific SDLC role (PM, Architect, Developer, QA, BA, PO)
2. Tasks are workflow templates that agents can execute
3. Templates provide consistent output formatting
4. Data files contain project-specific standards and references

---

==== FILE: .krci-ai/agents/dev.yaml ====
agent:
  identity:
    name: "Devon Coder"
    id: developer-v1
    version: "1.0.0"
    description: "Software Developer for implementation and code assistance"
    role: "Software Developer"
    goal: "Implement clean, efficient code with debugging and refactoring capabilities"
    icon: "ðŸ’»"

  activation_prompt:
    - Greet the user with your name and role, inform of available commands, then HALT to await instruction
    - Offer to help with development tasks but wait for explicit user confirmation
    - IMPORTANT!!! ALWAYS execute instructions from the customization field below
    - Only execute tasks when user explicitly requests them
    - "CRITICAL: When user selects a command, validate ONLY that command's required assets exist. If missing: HALT, report exact file, wait for user action."
    - "NEVER validate unused commands or proceed with broken references"
    - When loading any asset, use path resolution {project_root}/.krci-ai/{agents,tasks,data,templates}/*.md

  principles:
    - "Write clean, readable code following established patterns"
    - "Test thoroughly with comprehensive coverage"
    - "Document clearly for maintainability"
    - "Handle errors gracefully and provide meaningful feedback"

  customization: ""

  commands:
    help: "Show available commands"
    chat: "(Default) Development consultation and code assistance"
    review: "Review story technical requirements"
    plan-implementation: "Execute task plan-story-implementation"
    implement: "Implement new features"
    exit: "Exit Developer persona and return to normal mode"

  tasks:
    - ./.krci-ai/tasks/review-story-dev.md
    - ./.krci-ai/tasks/plan-story-implementation.md
    - ./.krci-ai/tasks/implement-feature.md

==== END FILE ====

==== FILE: tasks/review-story-dev.md ====
# Task: Review Story (Developer)

## Description

Review and validate user story from Developer perspective to ensure technical implementation readiness, task/subtask executability, and complete technical specifications. Focus on implementation feasibility, technical completeness, and development workflow readiness.

## Prerequisites

- [ ] **Story exists**: Target story file exists in `/docs/stories/` requiring developer technical review
- [ ] **Technical context**: Understanding of existing codebase, architecture, and technical standards
- [ ] **Implementation authority**: Developer expertise to validate technical feasibility and approach
- [ ] **Development environment**: Access to existing codebase and technical documentation

### Reference Assets

Dependencies:

- ./.krci-ai/templates/story.md
- ./.krci-ai/data/common/sdlc-framework.md

Validation: Verify all dependencies exist at specified paths before proceeding. HALT if any missing.

## Instructions

1. **Validate technical completeness**: Ensure story has sufficient detail for autonomous implementation
2. **Review task/subtask specificity**: Verify implementation steps are atomic, executable, and well-defined
3. **Check technical specifications**: Validate libraries, file paths, verification methods/commands, and dependencies are complete
4. **Assess implementation feasibility**: Confirm technical approach is viable and follows project standards
5. **Verify validation completeness**: Ensure testing and verification steps are comprehensive and executable

## Output Format

- **Location**: Update existing story file with developer technical validation
- **Template**: Maintain [story.md](./.krci-ai/templates/story.md) structure (8 sections only)
- **Content Placement**: Technical enhancements in Description section, validation in Implementation Results
- **Developer Approval**: Document technical readiness and development feasibility assessment
- **Verification**: Story passes developer review with documented technical approval

## Success Criteria

- [ ] **Technical implementation details complete**: All libraries, versions, file paths, and commands specified
- [ ] **Tasks/subtasks executable**: Each implementation step is atomic, specific, and actionable
- [ ] **Implementation autonomous**: Developer can implement without external technical consultations
- [ ] **Testing strategy comprehensive**: Validation commands and success criteria clearly defined
- [ ] **Architecture compliance**: Implementation approach follows project patterns and standards
- [ ] **Developer approval documented**: Technical readiness validation and approval recorded

## Execution Checklist

### Technical Completeness Assessment

- [ ] **Library specifications**: All required libraries include specific versions (e.g., `gopkg.in/yaml.v3 v3.0.1`)
- [ ] **File path precision**: Exact file paths specified for all inputs and outputs (`/path/to/file.ext`)
- [ ] **Verification executability**: Verification methods/commands are specific and executable without modification
- [ ] **Dependency clarity**: Technical dependencies clearly specified and available

### Task/Subtask Implementation Review

- [ ] **Atomic verification**: Each subtask represents single, executable action
- [ ] **Verification completeness**: Every subtask includes specific verification method/steps and success indicators
- [ ] **File target specificity**: Each task specifies exact files to create, modify, or validate
- [ ] **Validation integration**: Each subtask includes verification commands and success indicators

### Technical Architecture Validation

- [ ] **Project structure alignment**: Implementation fits existing directory and module organization
- [ ] **Pattern consistency**: Code follows established project patterns and conventions
- [ ] **Integration point clarity**: Clear identification of how new code integrates with existing systems
- [ ] **Performance consideration**: Implementation approach addresses performance requirements

### Implementation Feasibility Check

- [ ] **Technical viability**: Proposed approach is technically sound and implementable
- [ ] **Resource availability**: Required tools, libraries, and dependencies are accessible
- [ ] **Complexity assessment**: Implementation complexity matches story points and timeline
- [ ] **Risk identification**: Technical risks identified with mitigation approaches

### Quality Assurance Validation

- [ ] **Testing completeness**: QA checklist includes comprehensive testing requirements
- [ ] **Verification method**: Clear verification method provided (automated | semi-automated | manual) with commands where applicable
- [ ] **Success criteria**: Clear, measurable criteria for implementation completion
- [ ] **Error handling**: Testing includes error scenarios and edge cases

### Development Workflow Readiness

- [ ] **Implementation sequence**: Clear order of implementation tasks and dependencies
- [ ] **Development environment**: Environment setup and configuration requirements specified
- [ ] **Code review preparation**: Implementation approach enables effective code review
- [ ] **Documentation requirements**: Technical documentation needs clearly defined

## Content Guidelines

### Technical Implementation Principles for LLM Self-Evaluation

- **Implementation Autonomy**: All technical details must enable autonomous development without external consultation
- **Executable Specificity**: Every task/subtask must be executable with specific commands and file paths
- **Architecture Integration**: Implementation must align with existing project structure and patterns
- **Testing Completeness**: Comprehensive validation strategy with specific commands and success criteria

### LLM Error Prevention Checklist

- **Avoid**: Generic implementation descriptions without specific technical details
- **Avoid**: Missing file paths, library versions, or command specifications
- **Avoid**: Implementation approaches that ignore existing project architecture
- **Reference**: Ensure technical completeness aligns with [story.md](./.krci-ai/templates/story.md) template requirements

==== END FILE ====

==== FILE: tasks/plan-story-implementation.md ====
# Task: Plan Story Implementation

## Description

Comprehensive technical planning task for developers to analyze, validate, and enhance story implementation details before beginning development work. This task ensures complete technical understanding, detailed task/subtask planning, and implementation readiness with source code structure, libraries, patterns, schemas, and technical specifications.

## Prerequisites

- [ ] **Story exists**: Target story file exists in `/docs/stories/` requiring implementation planning
- [ ] **Developer role**: Task executed by development team member with implementation responsibility
- [ ] **Story approved**: Story has been reviewed and approved for implementation
- [ ] **Technical context**: Access to architecture documentation, existing codebase, and technical standards

### Reference Assets

Dependencies:

- ./.krci-ai/templates/story.md
- ./.krci-ai/data/common/sdlc-framework.md

Validation: Verify all dependencies exist at specified paths before proceeding. HALT if any missing.

## Instructions

1. **Validate story completeness**: Ensure story has sufficient technical detail for implementation
2. **Analyze technical requirements**: Deep dive into implementation needs, dependencies, and constraints
3. **Plan implementation approach**: Define specific technical approach, libraries, patterns, and structure
4. **Enhance task/subtask sections**: Create detailed, executable implementation steps
5. **Validate technical understanding**: Ensure complete comprehension before implementation begins
6. **Document implementation plan**: Create comprehensive technical specifications and approach

## Output Format

- **Location**: Update existing story file with implementation planning enhancements
- **Template**: Maintain [story.md](./.krci-ai/templates/story.md) structure (8 sections only)
- **Content Placement**: Technical details in Description section, enhanced tasks in Tasks/Subtasks section
- **Implementation Ready**: Story contains specific file paths, commands, and technical specifications
- **Verification**: Story enables autonomous development without additional technical consultation

## Success Criteria

- [ ] **Technical understanding complete**: Developer has full comprehension of implementation requirements
- [ ] **Implementation plan detailed**: All technical decisions documented and validated
- [ ] **Tasks/Subtasks enhanced**: Atomic, executable steps with complete specifications
- [ ] **Code structure planned**: Clear file structure, directories, and integration approach
- [ ] **Dependencies identified**: All libraries, tools, and external requirements specified
- [ ] **Validation strategy defined**: Complete testing approach with specific commands and criteria
- [ ] **Implementation ready**: Developer can begin work with confidence and clarity

## Execution Checklist

### Story Technical Validation

- [ ] **Story completeness check**: Verify story has business requirements and acceptance criteria
- [ ] **Technical gap analysis**: Identify missing technical details or specifications
- [ ] **Dependencies review**: Validate all technical dependencies are specified
- [ ] **Architecture alignment**: Confirm implementation approach fits project architecture

### Technical Requirements Analysis

- [ ] **Existing code analysis**: Review current codebase for integration points and patterns
- [ ] **Project structure mapping**: Analyze existing directory structure and identify where new components fit
- [ ] **Library specification**: Research and document required libraries with exact versions (format: `Library: package_name v1.2.3`)
- [ ] **Dependency compatibility**: Validate library compatibility with existing project dependencies
- [ ] **Pattern identification**: Define specific design patterns and approaches following project standards
- [ ] **Data structure planning**: Design schemas, models, and data flow with specific formats
- [ ] **Integration analysis**: Plan integration with existing systems and components
- [ ] **Configuration requirements**: Define environment setup, configuration files, and deployment needs
- [ ] **Performance considerations**: Identify performance requirements and optimization needs

### Implementation Approach Planning

- [ ] **Directory organization**: Plan specific directory structure following project patterns (src/, tests/, docs/, config/)
- [ ] **File structure design**: Define exact file paths and names for creation/modification (`/src/component/file.ext`)
- [ ] **Integration point mapping**: Identify specific integration points with existing codebase and APIs
- [ ] **Component architecture**: Define classes, functions, and component responsibilities with interfaces
- [ ] **Code reuse identification**: Identify opportunities to reuse existing components and shared utilities
- [ ] **Data flow design**: Map input/output flow and transformation logic with specific data formats
- [ ] **Error handling strategy**: Plan exception handling and error recovery following project patterns
- [ ] **Testing approach**: Define unit, integration, and validation testing strategy with existing frameworks
- [ ] **Security considerations**: Identify security requirements and implementation approach per project standards

### Task/Subtask Enhancement

- [ ] **Enhanced task formatting**: Use format "**Task N: Description (AC: X, Y)**" with clear acceptance criteria mapping
- [ ] **Atomic task breakdown**: Create single-responsibility implementation tasks with specific deliverables
- [ ] **Specific file targets**: Define exact file paths for creation/modification (`create file: /path/to/file.ext`)
- [ ] **Command specifications**: Include executable commands for each step (`run: command with args`)
- [ ] **Validation command integration**: Add verification commands for each task (`Command: \`test_command\``)
- [ ] **Purpose specification**: Document the purpose and responsibility of each file/component created
- [ ] **Dependency mapping**: Define dependencies between tasks using "depends on Task X completion" format
- [ ] **Success criteria**: Specify measurable completion criteria (file exists, tests pass, output matches)
- [ ] **Error recovery planning**: Define rollback steps if subtasks fail during implementation

### Technical Specifications Documentation

- [ ] **Libraries and versions**: Document all dependencies with specific versions
- [ ] **Configuration details**: Specify environment setup and configuration requirements
- [ ] **Database schemas**: Define data models, tables, and relationships if applicable
- [ ] **API specifications**: Document interfaces, endpoints, and data contracts
- [ ] **File formats**: Specify input/output formats, validation rules, and constraints
- [ ] **Command patterns**: Document CLI commands, scripts, and automation tools

### Implementation Validation Planning

- [ ] **Unit testing plan**: Define specific unit tests for each component
- [ ] **Integration testing**: Plan testing of component interactions and data flow
- [ ] **Validation commands**: Create specific commands to verify implementation correctness
- [ ] **Performance testing**: Define performance benchmarks and testing approach
- [ ] **Security validation**: Plan security testing and vulnerability assessment
- [ ] **End-to-end verification**: Create complete workflow validation steps

### Quality Assurance Integration

- [ ] **Code review preparation**: Identify areas requiring review and validation
- [ ] **Documentation requirements**: Plan code documentation and technical specifications
- [ ] **Compliance verification**: Ensure implementation meets project standards
- [ ] **Rollback planning**: Define rollback procedures if implementation fails
- [ ] **Monitoring setup**: Plan logging, monitoring, and observability integration
- [ ] **Deployment considerations**: Address deployment, configuration, and environment needs

## Content Guidelines

### Technical Planning Principles for LLM Self-Evaluation

- **Implementation-Ready Planning**: All technical decisions documented with specific details and rationale
- **Executable Task Enhancement**: Every task/subtask enhanced to be executable without additional research
- **Comprehensive Technical Validation**: Complete testing and verification approach planned for implementation
- **Architecture Integration**: All integration points, dependencies, and technical standards identified

### LLM Error Prevention Checklist

- **Avoid**: Generic planning without specific technical details (libraries, versions, file paths)
- **Avoid**: Task enhancement without validation commands and success criteria
- **Avoid**: Implementation planning that ignores existing project structure and patterns
- **Reference**: Use [story.md](./.krci-ai/templates/story.md) template for consistent enhancement formatting

==== END FILE ====

==== FILE: tasks/implement-feature.md ====
# Task: Implement Feature

## Description

Implement Story requirements according to Architecture specifications and coding standards, ensuring quality and maintaining system consistency. This task enables developers to systematically transform user stories into working code while maintaining Epic alignment and architectural compliance.

## Prerequisites

- [ ] **Story ready**: Story has been reviewed and validated with complete Tasks/Subtasks
- [ ] **Technical requirements clear**: All implementation details, file paths, and commands specified
- [ ] **Development environment**: Project codebase access and development tools configured
- [ ] **Dependencies available**: Required libraries, tools, and systems accessible

### Reference Assets

Dependencies:

- ./.krci-ai/data/common/sdlc-framework.md
- ./.krci-ai/data/coding-standards.md
- ./.krci-ai/data/best-practices.md

Validation: Verify all dependencies exist at specified paths before proceeding. HALT if any missing.

## Instructions

1. **Follow SDLC workflow**: Reference [sdlc-framework.md](./.krci-ai/data/common/sdlc-framework.md) for implementation dependencies and handoff requirements
2. **Apply coding standards**: Use guidelines from [coding-standards.md](./.krci-ai/data/coding-standards.md) and [best-practices.md](./.krci-ai/data/best-practices.md)
3. **Document progress**: Update Story file with implementation progress and results
4. **Ensure Story compliance**: Implement all Story acceptance criteria following Architecture specifications
5. **Maintain Epic traceability**: Preserve Story's Epic alignment and contribute to Epic completion

### Ready to Implement

Assume story is ready with complete Tasks/Subtasks for step-by-step execution.

## Implementation Approach

### STEP-BY-STEP Implementation

1. **Update story status** to "In Progress"
2. **Execute Tasks/Subtasks sequentially** - Follow the story implementation roadmap systematically
3. **Mark tasks as [x] immediately** when each task/subtask is completed
4. **Run tests and validation** - Check project documentation (README.md, Makefile, package.json) for test/build commands
5. **Update story status** to "Completed" when all tasks done
6. **ALWAYS populate Implementation Results section** with technical details, validation results, and business value

**Critical Documentation Requirements:**

- Mark individual tasks as [x] in real-time during implementation
- Change story status: "Approved" â†’ "In Progress" â†’ "Completed"
- Populate "## Implementation Results" section before completion
- Follow markdown linting rules (use #### headings, blank lines around lists)

## Output Format

- **Location**: Working code implementation with updated Story file in `/docs/stories/`
- **Story completion**: All empty sections populated with implementation details
- **Progress tracking**: Real-time updates to Tasks/Subtasks completion status
- **Quality documentation**: Completed QA Checklist and Implementation Results

## Success Criteria

- [ ] **Story implemented completely** - All Story acceptance criteria met with working code
- [ ] **Architecture compliant** - Implementation follows Architecture specifications and design patterns
- [ ] **Quality validated** - Code passes all tests, meets coverage requirements, and follows coding standards
- [ ] **Story updated** - Story file updated with implementation details, results, and completion status
- [ ] **System integration** - New code integrates properly with existing system without regressions
- [ ] **Documentation current** - Relevant documentation updated to reflect implementation changes
- [ ] **Epic progress** - Implementation contributes to Epic completion and traceability maintained

## Execution Checklist

### Setup

- [ ] **Locate story**: Find Story file in `/docs/stories/{epic_number}.{story_number}.story.md`
- [ ] **Review Tasks/Subtasks**: Understand the implementation roadmap

### Execute Tasks/Subtasks

- [ ] **Update status to "In Progress"**: Mark story as implementation started
- [ ] **Execute each subtask**: Work through Tasks/Subtasks sequentially, checking off completed items
- [ ] **Run specified commands**: Execute all commands specified in subtasks (e.g., `create file: path/file.ext`)
- [ ] **Validate deliverables**: Run verification commands specified in subtasks

### Complete Story

- [ ] **Run QA Checklist**: Execute all testing commands specified in story QA section
- [ ] **Verify acceptance criteria**: Confirm all acceptance criteria are met with working code

### Document Results

- [ ] **REQUIRED: Populate Implementation Results section**: Include summary, technical details, validation results, performance metrics, business value
- [ ] **Update status to "Completed"**: Mark story as complete in status table

## Implementation Guidelines

### Simple Execution Rules

- **Mark [x] immediately** when each task/subtask is completed
- **Update story status** at each phase: Approved â†’ In Progress â†’ Completed
- **Discover and run project's test/build commands** (check README.md, Makefile, package.json) to validate implementation
- **MUST populate Implementation Results** section with comprehensive details

### If Something Is Unclear

- **Stop implementation** - Do not guess or make assumptions
- **Use review-story task** - Get technical details clarified first
- **Resume when clear** - Continue once story has complete specifications

==== END FILE ====

# Shared Templates

==== FILE: templates/story.md ====
# Story {{story_number}}: {{story_title}}

## Status

| Field                  | Value                       |
|------------------------|-----------------------------|
| Status                 | {{status}}                  |
| Epic                   | {{epic_reference}}          |
| Priority               | {{priority}}                |
| Estimated Story Points | {{story_points}}            |
| Jira                   | {{jira_ticket}}             |

<!-- Status tracking and Epic/PRD traceability -->
<!-- Enables progress monitoring and dependency validation -->

<!-- Template Guidance:
Status Options: Draft -> Approved -> In Progress -> Done -> Completed
Epic Reference Example: "Epic 1 - Unified Agent Activation"
Priority Example: Critical, High, Medium, Low
Story Points Example: 1, 2, 3, 5, 8, 13 (Fibonacci scale)
Jira Example: "[EPMDEDP-15497](https://jira.example.com/browse/EPMDEDP-15497)"
 -->
## Dependencies

**Blocking:**
{{blocking_dependencies}}

**Blocked By:**
{{blocked_by_dependencies}}

**System/Test Dependencies:**
{{system_test_dependencies}}

<!-- Define precise dependencies for execution order and validation readiness -->

<!-- Template Guidance:
Format:
- Blocking: Items that depend on this story
- Blocked By: Items this story depends on (stories, approvals)
- System/Test: Environments, versions, stubs/mocks, fixtures

Examples:
- Blocking: Story 02.03 (depends on config file produced here)
- Blocked By: Story 02.01 (API contract), Security approval
- System/Test: Local IDP stub v1.2; Env var FEATURE_FLAG=on

DO:
- Use exact story numbers/links and system versions
- Include test doubles (stubs/mocks) to avoid external blockers
- State "None" explicitly if empty

DON'T:
- List vague dependencies (e.g., "backend work")
- Omit versions/links where relevant
- Depend on production-only services for acceptance
-->

## Story

**As a** {{persona}},
**I want** {{goal}},
**so that** {{business_value}}.

<!-- Standard story format focusing on persona, goal, and value -->
<!-- Must align with Epic Target Users and provide specific value -->

<!-- Template Guidance:
Story Example:
"As a Software Engineer,
I want a unified agent activation command in the IDE,
so that I can start using an agent without extra setup."

DO:
- Use persona from Epic (PRD-defined), not generic labels
- Make the goal specific and implementation-agnostic
- State tangible business/user value

DON'T:
- Invent personas not defined in PRD/Epic
- Use vague goals like "improve experience"
- Include solution details here
-->

## Acceptance Criteria

{{acceptance_criteria}}

<!-- Specific, testable conditions that define completion at story level -->
<!-- Story ACs SHOULD include verification commands and expected outputs -->

<!-- Required Structure for each criterion:
1. Scenario (Given/When/Then): Brief user/system flow to validate
2. Expected Behavior: Clear, observable outcome
3. Verification Command: Non-interactive command(s) with expected exit code/output
4. Files Created/Changed: Exact paths
5. Guardrails: NFR constraints applicable to this story (e.g., perf, security)
6. Test Data/Fixtures: Test users, payloads, seeds
7. Environment/Flags: Env vars, feature flags/toggles, mock/stub switches
8. Evidence: Required artifacts (logs, screenshots, run output)
9. Non-automatable at story-level: true/false (and rationale if true)
10. Traceability: Epic AC id(s), PRD BR/NFR
11. Out of Scope (optional): Clarify non-goals
-->

<!-- Template Guidance:
Acceptance Criteria Example:
"1. Unified activation command works in Cursor
   - Scenario: Given a valid agent config, when the user runs the activation command in Cursor, then the agent is activated
   - Expected Behavior: IDE shows active agent; no extra prompts
   - Verification Method: automated; run `python tools/activate_agent.py --ide=cursor --agent=architect` (Expect: exit 0, "Activated" in stdout)
   - Evidence: Command output artifact attached
   - Non-automatable at story-level: false
   - Files Created/Changed: `.ide/agents/architect.state`
   - Guardrails: Command completes in <= 3s on staging; no P0/P1 errors
   - Test Data/Fixtures: `fixtures/agents/architect.yaml`
   - Environment/Flags: `FEATURE_UNIFIED_ACTIVATION=1`
   - Traceability: Epic AC #1; BR2, BR3

2. Session continuity during workday
   - Scenario: Given a valid SSO session, when the user switches from Tool A to Tool B, then no re-auth is required within policy window
   - Expected Behavior: Continuous access without credential prompts
   - Verification Method: manual; steps 1â€“4 in staging with local IDP stub; observe uninterrupted access and audit log entry
   - Evidence: Screen recording + audit log excerpt
   - Non-automatable at story-level: true (follow-up: automate via Playwright against stub)
   - Files Created/Changed: None
   - Guardrails: Logout invalidates sessions in <= 60s
   - Test Data/Fixtures: Test user `sso_user@test`, seeded session
   - Environment/Flags: `SESSION_TTL_MINUTES=480`; local IDP stub enabled
   - Traceability: Epic AC #2; BR5, NFR-SEC

3. Failure handling
   - Scenario: Given the dependency is unavailable, when activation is attempted, then a clear, actionable error is shown and retry succeeds after recovery
   - Expected Behavior: Friendly error; subsequent retry succeeds
   - Verification Method: semi-automated; curl request to stubbed endpoint (Expect: HTTP 503 with error JSON), then after recovery (Expect: HTTP 200)
   - Evidence: Captured curl responses + log lines
   - Non-automatable at story-level: false
   - Files Created/Changed: `logs/activation.log`
   - Guardrails: No corrupted state; no secrets in logs
   - Test Data/Fixtures: Outage flag in stub config
   - Environment/Flags: `ACTIVATION_RETRY=1`
   - Traceability: Epic AC #3; NFR-UX1, NFR-REL"

Edge cases to cover (example list):
- Invalid agent name
- Missing permissions
- Network interruption mid-activation

DO:
- Make AC executable and non-interactive where possible
- Include expected outputs/exit codes
- Map each AC to Epic/PRD

DON'T:
- Use subjective language (e.g., "works well")
- Omit outputs or acceptance thresholds
- Depend on third-party production services for validation
-->

## Description

{{description}}

<!-- Context for why this story exists and its strategic importance -->
<!-- Provide background for implementation and Epic alignment -->

<!-- Template Guidance:
Content Focus:
- Why this story exists within the Epic
- Strategic importance and user/business context
- Relationships to other stories and architectural decisions
- Implementation philosophy or approach rationale (brief)
-->

## Tasks/Subtasks

{{tasks_subtasks}}

<!-- LLM-executable implementation plan with atomic tasks and validation -->
<!-- Each task maps to acceptance criteria with specific commands and file paths -->

<!-- Template Guidance:
Structure:
- [ ] Task 1: Short imperative description (AC: 1, 2)
  - [ ] Create/Edit file: `path/to/file`
  - [ ] Run: `non_interactive_command --args`
  - [ ] Verify: `assert_command/grep/exit 0`
  - [ ] Rollback Plan: Note revert steps if config/flags change
- [ ] Task 2: Short imperative description (AC: 3)

DO:
- Number tasks sequentially (1, 2, ...) and do not number subtasks
- Reference acceptance criteria ids for every task
- Use atomic subtasks with precise commands
- Mirror AC verification in tasks

DON'T:
- Skip numbering or mix numbering formats across tasks
- Create tasks that require human interpretation only
- Omit validation or rollback steps when needed
- Use vague action words like "handle" or "manage"
-->

## Implementation Results

{{implementation_results}}

<!-- Concrete outcomes populated AFTER completion (evidence-first) -->

<!-- Template Guidance:
Include:
- Evidence per AC (links to logs, screenshots, artifacts)
- Files created/updated with paths
- Commands executed with outputs
- Final validation summary

DO:
- Use past tense (Created, Implemented, Validated)
- Link evidence to specific AC ids
- Include actual command results/exit codes

DON'T:
- Populate before implementation
- Use future/planning language
- Omit evidence for any AC
-->

## QA Checklist

{{qa_checklist}}

<!-- Specific verification steps with commands and expected outputs -->
<!-- Enable automated testing and quality validation -->

<!-- Template Guidance:
QA Checklist Example:

### Functional
- [ ] Schema Validation: `python hack/validate-agents.py` (Expect: exit 0)
- [ ] File Existence: `ls -la assets/agents/architect.yaml` (Expect: file exists)
- [ ] Content Validation: `grep -q "identity" assets/agents/architect.yaml` (Expect: pattern found)

### Integration
- [ ] IDE Testing: Activate agent in Cursor (Expect: response within 5s)
- [ ] Cross-Platform: Validate on macOS/Linux/Windows (Expect: consistent behavior)

### Security & Privacy
- [ ] Secrets: No tokens/secrets in logs
- [ ] Auth: No P0/P1 security findings in changed scope

### Accessibility (if UI)
- [ ] Keyboard navigation and focus order
- [ ] Contrast ratio meets baseline

DO:
- Group tests by category; include expected outputs
- Keep checks non-interactive where possible
- Align with AC guardrails

DON'T:
- Use subjective testing criteria
- Omit expected outputs or success indicators
- Depend on production-only services
-->

<!-- Definition of Ready (DoR) â€“ optional checklist (commented)
- Persona exists in Epic/PRD and matches this story
- Dependencies resolved or test doubles defined
- Test data/fixtures identified and accessible
- AC are executable, non-interactive, and map to tasks
- Feature flags/rollout strategy identified
- Small enough to complete within one sprint (INVEST)
-->

<!-- Definition of Done (DoD) â€“ optional checklist (commented)
- All AC pass with evidence linked in Implementation Results
- Unit/integration tests updated/added
- Docs/README updated if applicable
- Feature flags default state decided; rollback plan noted
- Security/privacy checks completed; no P0/P1 issues
-->

==== END FILE ====

# Reference Data

==== FILE: data/best-practices.md ====
# Best Practices

## Development

- Start with simplest solution
- Refactor regularly
- Use version control effectively

## Code Quality

- Follow SOLID principles
- Minimize dependencies
- Write self-documenting code

## Collaboration

- Review code thoroughly
- Share knowledge
- Communicate changes clearly

==== END FILE ====

==== FILE: data/coding-standards.md ====
# Coding Standards

## Code Style

- Use consistent indentation
- Follow language conventions
- Keep functions small and focused

## Quality

- Write clear, readable code
- Add meaningful comments
- Handle errors properly

## Testing

- Write unit tests
- Test edge cases
- Maintain test coverage

==== END FILE ====

